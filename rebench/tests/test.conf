# Config file for ReBench
# Config format is YAML (see http://yaml.org/ for detailed spec)

# this run definition will be choosen if no parameters are given to rebench.py
standard_run: Test
standard_data_file: 'tests/test.data'

# reporting should enable the configuration of the format of the out put
# REM: not implement yet (STEFAN: 2011-01-19)
reporting:
    # results can also be reported to a codespeed instance
    # see: https://github.com/tobami/codespeed
    # codespeed:
    #     url: http://localhost:8000/result/add-multiple/
    #     project: test
    #     # other details like commitid are required to be given as parameters
    csv_file: test.csv
    csv_locale: de_DE.UTF-8
    csv_raw:  tests/test.data.csv
    email:    rebench@stefan-marr.de

# settings and requirements for statistic evaluation
statistics:
    min_runs: 5
    max_runs: 10
    confidence_level: 0.95     #all measurments lie with a probability of 95% in the convidence interval 
    error_margin: 0.005        #the size of the confidence interfal should not be larger the 0.5% of the mean value
    min_runtime: 800           # give a warning if average runtime is below this value 
 
# settings for quick runs, useful for fast feedback during experiments
quick_runs:
    min_runs: 3
    max_runs: 10
    max_time: 60   # time in seconds

# definition of benchmark suites
# settings in the benchmark suite will be overriden by similar settings of the VM
benchmark_suites:
    TestSuite1:
        performance_reader: TestVMPerformance
        # location: /Users/...
        command: TestBenchMarks %(benchmark)s %(input)s %(variable)s
        input_sizes: [2, 10]
        benchmarks:
            - Bench1
            - Bench2:
                extra_args: 6
        max_runtime: 1 # specifies the maximum runtime in seconds
        variable_values: # this is an other dimension, over which the runs need to be varied
            - val1
            - val2
    TestSuite2:
        performance_reader: TestVMPerformance
        command: TestBenchMarks %(benchmark)s %(input)s %(variable)s
        input_sizes: [100, 1000]
        cores: [7, 13]
        benchmarks:
            - Bench1:
                extra_args: "%(cores)s 3000"
            - Bench2
    TestBrokenCommandFormatSuite:
        performance_reader: TestVMPerformance
        command: TestBenchMarks %(benchmark) %(input) %(variable) # conversion is not indicated, needs to be %(benchmark)s to indicate string conversion
        input_sizes: [100, 1000]
        cores: [7, 13]
        benchmarks: [Bench1]
    TestBrokenCommandFormatSuite2:
        performance_reader: TestVMPerformance
        command: " %(benchmark) -i 3000s " # conversion is not indicated, needs to be %(benchmark)s to indicate string conversion
        input_sizes: [1]
        cores: [1]
        benchmarks: [Bench1]
    TestWarningForRemovedUlimitUsageSuite:
        performance_reader: TestVMPerformance
        command: TestBenchMarks
        benchmarks: [Bench1]
        ulimit: 1
    TestCaliper:
        performance_reader: CaliperPerformance
        command: " %(benchmark)s -i macro -l 3000s "
        input_sizes: [1]
        cores: [1]
        benchmarks: AliasMOP.ExampleBench

# VMs have a name and are specified by a path and the binary to be executed
# optional: the number of cores for which the runs have to be executed
virtual_machines:
    TestRunner1:
        path: tests
        binary: test-vm1.py %(cores)s
        cores: [1, 4]
    TestRunner2:
        path: tests
        binary: test-vm2.py
    Caliper:
        path: tests
        binary: cat.sh caliper.output

# define the benchmarks to be executed for a re-executable benchmark run
# special definitions done here should override benchmark suite definitions, and
# VM definitions
run_definitions:
    Test:
        description: >
            This run definition is used for testing.
            It should try all possible settings and the generated out
            will be compared to the expected one by the unit test(s)
        actions: benchmark #can be a list
        benchmark:       #
            - TestSuite1
            - TestSuite2
        statistics:
            min_runs: 1
            max_runs: 1
        data_file: TestTest.data.data
        reporting:
            csv_file: TestTest.data.data.csv
        executions:
            # List of VMs and Benchmarks/Benchmark Suites to be run on them
            # benchmarks define here will override the ones defined for the whole run
            
            #the following example is equivalent to the global run definition,
            #but needs to be tested...
            - TestRunner1:
                benchmark: TestSuite1
                cores: [42]
            - TestRunner1:
                benchmark: TestSuite2
            - TestRunner2
        # visualization:
        #     fileName:   test_%s_%s.png       # fileName needs placeholders for the dimentions specified by separateBy
        #     separateBy: [cores, input_sizes] # generates different diagrams per 'cores' value
        #     groupBy: variable_values
        #     sortBy: {stats : median}  # median is used for sorting, since the mean would emphasize outliers much more
        #     criterion: total # use only the total cirterion for the diagrams
        #     columnName: '{0} on {1}' # is determined from the remaining characteristics with are indexable
        #     title: 'Test Plot: Just Testing'
        #     labelXAxis: ''
        #     labelYAxis: ''
    Test-variable-values:
        description: to test for a bug
        actions:    benchmark
        benchmark:  TestSuite2
        executions: TestRunner2
    TestProfiling:
        description: >
            This run is used to test the profiling run type
        actions: profile
        benchmark: TestSuite1
        input_sizes: 1
        executions:
            - CSOM
    TestBrokenCommandFormat:
        description: to test for a proper error when conversions are not properly indicated in the config
        actions:    benchmark
        benchmark:  TestBrokenCommandFormatSuite
        executions: TestRunner2
    TestBrokenCommandFormat2:
        description: to test for a proper error when conversions are not properly indicated in the config
        actions:    benchmark
        benchmark:  TestBrokenCommandFormatSuite2
        executions: TestRunner2
    TestWarningForRemovedUlimitUsage:
        description: make sure the usage of ulimit in a configuration triggers a warning
        actions:     benchmark
        benchmark:   TestWarningForRemovedUlimitUsageSuite
        executions:  TestRunner2
        input_sizes: 1
    TestCaliper:
        description: test Caliper support
        actions:     benchmark
        benchmark:   TestCaliper
        executions:  Caliper
        
